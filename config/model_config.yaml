# Model configurations
models:
  # LLM models available for text generation tasks
  llm_models:
    gpt4o:
      name: "gpt-4o"
      type: "api"
      parameters:
        temperature: 0.8
        top_p: 0.95
        max_new_tokens: 150

    llama2_70b:
      name: "meta-llama/Llama-2-70b-chat-hf"
      type: "vllm"
      model_path: "meta-llama/Llama-2-70b-chat-hf"
      parameters:
        temperature: 0.8
        top_p: 0.95
        max_new_tokens: 150
        tensor_parallel_size: 4

    llama2_13b:
      name: "meta-llama/Llama-2-13b-chat-hf"
      type: "vllm"
      model_path: "models/llama2-13b"
      parameters:
        temperature: 0.8
        top_p: 0.95
        max_new_tokens: 150

    qwen25_32b:
      name: "Qwen/Qwen2.5-32B-Instruct"
      type: "vllm"
      model_path: "Qwen/Qwen2.5-32B-Instruct"
      parameters:
        temperature: 0.8
        top_p: 0.95
        max_new_tokens: 150
        tensor_parallel_size: 4

    phi35:
      name: "microsoft/phi-3.5"
      type: "transformers"
      parameters:
        temperature: 0.8
        top_p: 0.95
        max_new_tokens: 150

    llama3_70b:
      name: "meta-llama/Llama-3.3-70B-Instruct"
      type: "vllm"
      model_path: "meta-llama/Llama-3.3-70B-Instruct"
      parameters:
        temperature: 0.8
        top_p: 0.95
        max_new_tokens: 150
        tensor_parallel_size: 6

    qwen25_72b:
      name: "Qwen/Qwen-2.5-72B-Instruct"
      type: "vllm"
      model_path: "Qwen/Qwen-2.5-72B-Instruct"
      parameters:
        temperature: 0.8
        top_p: 0.95
        max_new_tokens: 150
        tensor_parallel_size: 6

    qwen2_70b:
      name: "Qwen/Qwen-2-70B-Instruct"
      type: "vllm"
      model_path: "Qwen/Qwen-2-70B-Instruct"
      parameters:
        temperature: 0.8
        top_p: 0.95
        max_new_tokens: 150
        tensor_parallel_size: 6

  # Model serving configuration
  serving:
    vllm:
      tensor_parallel_size: 4  # Updated for larger models
      gpu_memory_utilization: 0.99  # Slightly reduced to prevent OOM
      max_num_batched_tokens: 8192
      trust_remote_code: true
      dtype: "float16"
      quantization:
        enabled: false
        bits: 4
        group_size: 128
        use_double_quant: true  # Enable double quantization for memory efficiency

  image_annotation:
    name: "phi-3.5-vision-instruct"  # Alternative: "BLIP-2"
    parameters:
      max_new_tokens: 100
      temperature: 0.7
  
  # Text generation model selection
  question_generation:
    default_model: "gpt4o"  # Can be any model defined in llm_models
    parameters:
      temperature: 0.8
      top_p: 0.95
      max_new_tokens: 150
      num_beams: 4
      no_repeat_ngram_size: 3
      early_stopping: true
    fallback_model: "phi35"

  self_instruct:
    default_model: "gpt4o"  # Can be any model defined in llm_models
    parameters:
      temperature: 0.9
      top_p: 0.95
      max_new_tokens: 200
      presence_penalty: 0.2
      frequency_penalty: 0.3
    fallback_model: "phi35"

  # Document processing models
  document_understanding:
    layout_model: "microsoft/layoutlm-base-uncased"
    ocr_model: "tesseract"
    table_model: "microsoft/table-transformer-detection"
    parameters:
      max_seq_length: 512
      doc_stride: 128
      batch_size: 16

  # Image processing models
  image_processing:
    caption_model: "Salesforce/blip2-opt-2.7b"
    visual_qa_model: "Salesforce/blip2-opt-2.7b-vqa"
    object_detection: "facebook/detr-resnet-50"
    parameters:
      max_length: 100
      num_beams: 4
      min_length: 10
      repetition_penalty: 1.5

  # Quality assessment models
  quality_assessment:
    text_relevance: "bert-base-uncased"
    answer_validation: "roberta-large-mnli"
    parameters:
      batch_size: 32
      max_length: 512
      threshold: 0.7

# Model caching and optimization
optimization:
  use_cuda: true
  mixed_precision: true
  cache_dir: "cache/models"
  load_in_8bit: true
  use_better_transformers: true
  max_memory:
    gpu: "12GB"
    cpu: "32GB"

# Default model parameters
default_parameters:
  max_length: 512
  temperature: 0.8
  top_p: 0.95
  top_k: 50
  repetition_penalty: 1.2
  length_penalty: 1.0
  batch_size: 16
  gradient_accumulation_steps: 1

# Model-specific prompts and templates
prompts:
  question_generation:
    base_template: |
      Generate a question based on the following content:
      Content: {context}
      Answer: {answer}
      Requirements:
      1. Question must be answerable using only the given content
      2. Question should be clear and specific
      3. Question should focus on the provided answer
      Generate question:

  visual_question:
    base_template: |
      Generate a question about the visual content described below:
      Visual description: {description}
      Context: {context}
      Focus on: {focus}
      Generate a clear and specific question:

  document_question:
    base_template: |
      Generate a question about the following content from a {doc_type}:
      Content: {content}
      Focus area: {focus}
      Document section: {section}
      Generate a question that: